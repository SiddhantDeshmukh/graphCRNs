{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype a ResNet\n",
    "import numpy as np\n",
    "from nn_models import simple_dnn\n",
    "from abundances import *\n",
    "from typing import Dict, List\n",
    "import glob\n",
    "import vaex\n",
    "import tensorflow as tf\n",
    "import tensorflow_gnn as gnn\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "from gcrn.helper_functions import number_densities_from_abundances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Input: abundances, gas density, temperature (10 quantities for chem1)\n",
    "# Output: equilibrium number densities (8 quantities for chem1)\n",
    "\"\"\"\n",
    "TODO\n",
    "  - Write architectures for\n",
    "    - ResNet\n",
    "  - Check normal ML algs\n",
    "    - SVMs\n",
    "    - Decision Trees/Random Forests\n",
    "    - XGBoost\n",
    "  - Functions to standardise input\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Pipeline overview:\n",
    "  - Read in .parquet files\n",
    "  - Generate dataset (extract density, temperature, EQ number densities)\n",
    "  - Determine and calculate abundances based on keys\n",
    "  - Sort input abundance array and output EQ array alphabetically\n",
    "  - Pass into model with goal:\n",
    "    - from density, temperature and abundance, map to output EQ number densities\n",
    "  - Loss function analysis\n",
    "\"\"\"\n",
    "\n",
    "def load_dataset(directory: str, suffix=\"*.parquet\"):\n",
    "  files = [f\"file://{f}\" for f in glob.glob(f\"{directory}/{suffix}\")]\n",
    "  print(files)\n",
    "  with make_batch_reader(files) as reader:\n",
    "    dataset = make_petastorm_dataset(reader)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    tensor = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "      sample = sess.run(tensor)\n",
    "      print(sample.id)\n",
    "\n",
    "def load_dataframe(path: str):\n",
    "  return vaex.open(path)\n",
    "\n",
    "def minmax(arr):\n",
    "  return np.nanmin(arr), np.nanmax(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cemp_dir = \"/media/sdeshmukh/Crucial X6/mean_chemistry/combined_cemp\"\n",
    "suffix = \"*chem1*.txt\"\n",
    "\n",
    "species = [\"H\", \"H2\", \"C\", \"O\", \"CO\", \"CH\", \"OH\", \"M\"]\n",
    "chem_keys = [f\"{s}_EQ\" for s in [\"H\", \"H2\", \"C\", \"O\", \"CO\", \"CH\", \"OH\", \"M\"]]\n",
    "input_keys = [*[f\"A_{s}\" for s in species], \"density\", \"temperature\"]\n",
    "columns = [*chem_keys, \"density\", \"temperature\"]\n",
    "print(columns)\n",
    "print(input_keys)\n",
    "test_suffix = \"d3t63g40mm30chem1_04*.parquet\"  # load one file\n",
    "test_file = \"d3t63g40mm30chem1_087.parquet\"\n",
    "df = load_dataframe(f\"{cemp_dir}/{test_file}\")\n",
    "# dataset = load_dataset(cemp_dir, suffix=test_suffix)\n",
    "# load_dataset(cemp_dir, test_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map density and abundances to number densities\n",
    "# mass_hydrogen = 1.67262171e-24  # g\n",
    "# abundance_values = np.array([v for v in mm00_abundances.values()])\n",
    "# percentage_hydrogen = 10**mm00_abundances['H'] / np.sum(10**abundance_values)\n",
    "# hydrogen_density = df['density'] / (percentage_hydrogen * mass_hydrogen)\n",
    "# # Index abundance values as species\n",
    "# abundance_inputs = np.array([mm00_abundances[s] for s in species])\n",
    "# number_densities = 10**(np.log10(hydrogen_density) + abundance_inputs - 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just pass the abundances in\n",
    "abundances = np.array([mm30a04_abundances[s] for s in species])\n",
    "new_cols = [f\"A_{s}\" for s in species]\n",
    "\n",
    "n_elements = df['density'].shape[0]\n",
    "for i, k in enumerate(new_cols):\n",
    "  df[k] = np.repeat(abundances[i], n_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[input_keys]\n",
    "data[\"density\"] = np.log10(data[\"density\"])\n",
    "label = df[chem_keys]\n",
    "for k in chem_keys:\n",
    "  label[k] = np.log10(label[k])\n",
    "\n",
    "data_rest, data_test, label_rest, label_test = train_test_split(data.values, label.values, test_size=0.25, random_state=42)\n",
    "data_train, data_val, label_train, label_val = train_test_split(data_rest, label_rest, test_size=0.1, random_state=42)\n",
    "print(\"Train\", data_train.shape, label_train.shape)\n",
    "print(\"Val\", data_val.shape, label_val.shape)\n",
    "print(\"Test\", data_test.shape, label_test.shape)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in chem_keys:\n",
    "  print(k, np.any(np.isnan(label[k].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 64)                704       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,544\n",
      "Trainable params: 9,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "2055/2055 [==============================] - 14s 7ms/step - loss: 9.1619 - mae: 2.2630 - val_loss: 4.0147 - val_mae: 1.5498\n",
      "Epoch 2/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 5.0838 - mae: 1.7627 - val_loss: 5.9031 - val_mae: 1.6750\n",
      "Epoch 3/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 3.5135 - mae: 1.4505 - val_loss: 6.9119 - val_mae: 1.8538\n",
      "Epoch 4/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 2.9832 - mae: 1.3278 - val_loss: 7.7173 - val_mae: 2.0304\n",
      "Epoch 5/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 2.5797 - mae: 1.2224 - val_loss: 8.2742 - val_mae: 2.1397\n",
      "Epoch 6/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 2.2527 - mae: 1.1264 - val_loss: 8.2092 - val_mae: 2.1360\n",
      "Epoch 7/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.9981 - mae: 1.0425 - val_loss: 4.2230 - val_mae: 1.5168\n",
      "Epoch 8/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.8046 - mae: 0.9700 - val_loss: 3.9591 - val_mae: 1.5386\n",
      "Epoch 9/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.6642 - mae: 0.9075 - val_loss: 4.8359 - val_mae: 1.7604\n",
      "Epoch 10/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5900 - mae: 0.8676 - val_loss: 4.3444 - val_mae: 1.6548\n",
      "Epoch 11/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5565 - mae: 0.8484 - val_loss: 4.1336 - val_mae: 1.5752\n",
      "Epoch 12/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5394 - mae: 0.8395 - val_loss: 4.1040 - val_mae: 1.5548\n",
      "Epoch 13/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5271 - mae: 0.8345 - val_loss: 4.1250 - val_mae: 1.5682\n",
      "Epoch 14/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5208 - mae: 0.8314 - val_loss: 4.5177 - val_mae: 1.6767\n",
      "Epoch 15/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5134 - mae: 0.8285 - val_loss: 4.7903 - val_mae: 1.7409\n",
      "Epoch 16/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5077 - mae: 0.8261 - val_loss: 5.0472 - val_mae: 1.7822\n",
      "Epoch 17/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.5024 - mae: 0.8237 - val_loss: 5.1279 - val_mae: 1.7982\n",
      "Epoch 18/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4981 - mae: 0.8220 - val_loss: 5.7562 - val_mae: 1.9151\n",
      "Epoch 19/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4943 - mae: 0.8206 - val_loss: 5.9894 - val_mae: 1.9503\n",
      "Epoch 20/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4909 - mae: 0.8193 - val_loss: 6.0341 - val_mae: 1.9588\n",
      "Epoch 21/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4876 - mae: 0.8180 - val_loss: 6.0014 - val_mae: 1.9518\n",
      "Epoch 22/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4848 - mae: 0.8168 - val_loss: 5.9919 - val_mae: 1.9485\n",
      "Epoch 23/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4836 - mae: 0.8160 - val_loss: 6.0080 - val_mae: 1.9518\n",
      "Epoch 24/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4805 - mae: 0.8150 - val_loss: 6.0136 - val_mae: 1.9543\n",
      "Epoch 25/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4784 - mae: 0.8141 - val_loss: 6.0059 - val_mae: 1.9525\n",
      "Epoch 26/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4763 - mae: 0.8134 - val_loss: 6.0009 - val_mae: 1.9505\n",
      "Epoch 27/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4744 - mae: 0.8124 - val_loss: 6.0268 - val_mae: 1.9600\n",
      "Epoch 28/500\n",
      "2055/2055 [==============================] - 14s 7ms/step - loss: 1.4736 - mae: 0.8122 - val_loss: 6.0423 - val_mae: 1.9577\n",
      "Epoch 29/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4723 - mae: 0.8116 - val_loss: 6.0812 - val_mae: 1.9693\n",
      "Epoch 30/500\n",
      "2055/2055 [==============================] - 14s 7ms/step - loss: 1.4703 - mae: 0.8107 - val_loss: 6.1728 - val_mae: 1.9858\n",
      "Epoch 31/500\n",
      "2055/2055 [==============================] - 13s 7ms/step - loss: 1.4688 - mae: 0.8103 - val_loss: 6.2183 - val_mae: 1.9888\n",
      "Epoch 32/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4683 - mae: 0.8101 - val_loss: 6.1781 - val_mae: 1.9826\n",
      "Epoch 33/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4669 - mae: 0.8093 - val_loss: 6.1373 - val_mae: 1.9773\n",
      "Epoch 34/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4652 - mae: 0.8086 - val_loss: 6.0788 - val_mae: 1.9661\n",
      "Epoch 35/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4645 - mae: 0.8085 - val_loss: 6.0648 - val_mae: 1.9630\n",
      "Epoch 36/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4630 - mae: 0.8080 - val_loss: 6.0579 - val_mae: 1.9617\n",
      "Epoch 37/500\n",
      "2055/2055 [==============================] - 13s 6ms/step - loss: 1.4624 - mae: 0.8077 - val_loss: 6.0321 - val_mae: 1.9566\n",
      "Epoch 38/500\n",
      "2055/2055 [==============================] - 13s 7ms/step - loss: 1.4600 - mae: 0.8068 - val_loss: 6.0394 - val_mae: 1.9566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcda060a5b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def mlp():\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=(10,), activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(32, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(32, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(8, activation=\"linear\"),\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "def mlp_dropout():\n",
    "  # Same as mlp() but with Dropout layers between each Dense layer\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(64, input_shape=(10,), activation=\"sigmoid\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"sigmoid\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"sigmoid\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(8, activation=\"linear\"),\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "def cnn_1d():\n",
    "  # 1D CNN\n",
    "  model = keras.Sequential([\n",
    "    # # Input\n",
    "    # keras.layers.Dense(32, input_shape=(10,), activation=\"sigmoid\"),\n",
    "    # Convolutional Layers\n",
    "    keras.layers.Conv1D(32, 4, input_shape=(10, 1), activation=\"sigmoid\", padding=\"same\"),\n",
    "    keras.layers.Conv1D(64, 4, activation=\"sigmoid\"),\n",
    "    keras.layers.MaxPooling1D(2),\n",
    "    keras.layers.Flatten(),\n",
    "    # Output\n",
    "    keras.layers.Dense(8, activation=\"linear\")\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def encoder_decoder():\n",
    "  model = keras.Sequential([\n",
    "    # Encoder\n",
    "    keras.layers.Dense(128, input_shape=(10,), activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(64, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(32, activation=\"sigmoid\"),\n",
    "    # Decoder\n",
    "    keras.layers.Dense(32, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(64, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(128, activation=\"sigmoid\"),\n",
    "    # Output\n",
    "    keras.layers.Dense(8, activation=\"linear\")\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "def random_forest():\n",
    "  model = RandomForestRegressor(n_estimators=100, criterion=\"mae\", verbose=1)\n",
    "  return model\n",
    "\n",
    "# TensorFlow\n",
    "# model = mlp()\n",
    "model = mlp_dropout()\n",
    "# model = encoder_decoder()\n",
    "# model = cnn_1d()\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "model.fit(data_train, label_train, batch_size=512, epochs=500,\n",
    "          validation_data=(data_val, label_val),\n",
    "          callbacks=keras.callbacks.EarlyStopping(restore_best_weights=True,\n",
    "                                                  patience=30),\n",
    "          verbose=1)\n",
    "\n",
    "# # Sklearn\n",
    "# model = random_forest()\n",
    "# model.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12176/12176 [==============================] - 26s 2ms/step - loss: 1.3483 - mae: 0.7495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3483366966247559, 0.7495296001434326]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "(H_EQ): Prediction = 16.326. Truth = 15.855. P - T = 0.47\n",
      "(H2_EQ): Prediction = 13.413. Truth = 15.381. P - T = -1.97\n",
      "(C_EQ): Prediction = 9.492. Truth = 6.195. P - T = 3.30\n",
      "(O_EQ): Prediction = 10.418. Truth = 9.770. P - T = 0.65\n",
      "(CO_EQ): Prediction = 7.044. Truth = 9.488. P - T = -2.44\n",
      "(CH_EQ): Prediction = 6.360. Truth = 5.083. P - T = 1.28\n",
      "(OH_EQ): Prediction = 7.893. Truth = 9.679. P - T = -1.79\n",
      "(M_EQ): Prediction = 15.350. Truth = 15.078. P - T = 0.27\n"
     ]
    }
   ],
   "source": [
    "def check_random_idx(model, X_test, y_test, seed=42):\n",
    "  if seed:\n",
    "    np.random.seed(seed)\n",
    "  idx = np.random.randint(0, X_test.shape[0])\n",
    "  predictions = model.predict(X_test[idx:idx+10])\n",
    "  truths = y_test[idx:idx+10]\n",
    "\n",
    "  return predictions, truths\n",
    "\n",
    "predictions, truths = check_random_idx(model, data_test, label_test, seed=None)\n",
    "for i, s in enumerate(chem_keys):\n",
    "  for j in range(1):\n",
    "    print(f\"({s}): Prediction = {predictions[j, i]:.3f}. Truth = {truths[j, i]:.3f}. P - T = {predictions[j, i] - truths[j, i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9236690f3a11807748b7e4a46e6b56e0a64b7fb380b42be31cedaa8c090196a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
